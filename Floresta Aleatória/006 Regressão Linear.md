- Introdu√ß√£o √† regress√£o linear
- GLM
- M√≠nimos Quadrados N√£o Lineares (NLS)
- Correla√ß√£o
[6.1 - Introduction to GLMs | STAT 504 (psu.edu)](https://online.stat.psu.edu/stat504/lesson/6/6.1#:~:text=The%20term%20%22general%22%20linear%20model,%28with%20fixed%20effects%20only%29.)
- Regress√£o Log√≠stica
- Exemplos
- [(1) Kareem Carr | Data Scientist | üìäüìà on X: "Here are three different ways of thinking about linear regression and why it works. https://t.co/y4po9BpaCD" / X (twitter.com)](https://twitter.com/kareem_carr/status/1688225360184512513)
- Log transforming
- Regress√£o M√∫ltipla
[Overview of Multivariate Analysis | What is Multivariate Analysis? (mygreatlearning.com)](https://www.mygreatlearning.com/blog/introduction-to-multivariate-analysis/#:~:text=Multivariate%20analysis%20(MVA)%20is%20a,analyzed%20simultaneously%20with%20other%20variables.)

[(1) Cameron Patrick on X: "sigh, the original meaning of "multivariate" in statistics is completely a lost cause now hey https://t.co/9GFzwV6t3X" / X (twitter.com)](https://twitter.com/camjpatrick/status/1696744910475690456)

[BIOS¬≤ Education resources - Generalized Linear Models for Community Ecology (bios2.github.io)](https://bios2.github.io/posts/2021-07-19-glm-community-ecology/)

[regression - Why is multicollinearity different than correlation? - Cross Validated (stackexchange.com)](https://stats.stackexchange.com/questions/545148/why-is-multicollinearity-different-than-correlation#:~:text=Multicollinearity%20may%20occur%20even%20when,sum%20of%20the%20other%20predictors.)

Os coeficientes.
[Pearson Correlation Coefficient (r) | Guide & Examples (scribbr.com)](https://www.scribbr.com/statistics/pearson-correlation-coefficient/)

[[M√≠nimos Quadrados]]

[[Vari√°veis Censuradas e Truncadas]]

# M√≠nimos Quadrados N√£o Lineares

**M√≠nimos Quadrados N√£o Lineares (NLS):**

O m√©todo de M√≠nimos Quadrados N√£o Lineares (NLS) √© usado para ajustar modelos matem√°ticos n√£o lineares a dados observados. Ele busca encontrar os par√¢metros do modelo que minimizam a soma dos quadrados das diferen√ßas entre os valores observados e os valores previstos pelo modelo. Esse m√©todo √© amplamente utilizado quando se tem uma fun√ß√£o matem√°tica que descreve a rela√ß√£o entre as vari√°veis, mas a rela√ß√£o n√£o √© linear. O NLS √© sens√≠vel aos valores iniciais dos par√¢metros e pode requerer ajustes cuidadosos para convergir para uma solu√ß√£o.
√â uma extens√£o dos m√≠nimos quadrados lineares



1. **Residual Standard Error:** Tamb√©m conhecido como erro padr√£o residual ou erro padr√£o dos res√≠duos, isso representa a estimativa da variabilidade n√£o explicada pelo modelo. Em outras palavras, √© uma medida da dispers√£o dos res√≠duos, que s√£o as diferen√ßas entre os valores observados e os valores previstos pelo modelo. Quanto menor o valor do erro padr√£o residual, melhor o modelo ajusta-se aos dados, pois os res√≠duos estar√£o mais pr√≥ximos de zero.
    
2. **53.75:** Esse √© o valor num√©rico do erro padr√£o residual. No contexto da sua an√°lise, esse valor indica a dispers√£o m√©dia dos res√≠duos ao redor das previs√µes do modelo. Quanto maior o valor, maior a variabilidade n√£o explicada pelo modelo, e vice-versa.
    
3. **on 1097016 degrees of freedom:** Aqui, "degrees of freedom" (graus de liberdade) refere-se √† quantidade de informa√ß√£o dispon√≠vel para estimar os par√¢metros do modelo. Quanto mais graus de liberdade, mais informa√ß√µes voc√™ tem para ajustar o modelo. Nesse caso, voc√™ tem 1.097.016 graus de liberdade para o c√°lculo do erro padr√£o residual. Essa quantidade est√° relacionada ao tamanho do seu conjunto de dados e ao n√∫mero de par√¢metros estimados pelo modelo.




[Numeracy, Maths and Statistics - Academic Skills Kit (ncl.ac.uk)](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html#:~:text=%C2%AFy)
2.-,R%202%20%3D%201%20%E2%88%92%20sum%20squared%20regression%20(SSR)%20total,from%20the%20mean%20all%20squared.)
O coeficiente de determina√ß√£o \(R^2\) √© uma m√©trica estat√≠stica utilizada em regress√£o linear para avaliar o ajuste do modelo aos dados. Ele mede a propor√ß√£o da variabilidade total dos valores observados que √© explicada pelo modelo de regress√£o. O \(R^2\) varia de 0 a 1, onde 0 indica que o modelo n√£o explica nenhuma varia√ß√£o dos dados e 1 indica que o modelo explica toda a varia√ß√£o dos dados.

O \(R^2\) m√∫ltiplo (ou \(R^2\) de m√∫ltipla regress√£o) √© usado quando se ajusta um modelo de regress√£o m√∫ltipla com v√°rias vari√°veis independentes. Ele mede a propor√ß√£o da variabilidade total da vari√°vel dependente que √© explicada pelas vari√°veis independentes do modelo. Em ess√™ncia, o \(R^2\) m√∫ltiplo indica o qu√£o bem as vari√°veis independentes explicam as varia√ß√µes na vari√°vel dependente.

O \(R^2\) ajustado, por outro lado, corrige o \(R^2\) m√∫ltiplo para o n√∫mero de vari√°veis independentes no modelo e o tamanho da amostra. O \(R^2\) m√∫ltiplo tende a aumentar quando mais vari√°veis independentes s√£o adicionadas ao modelo, mesmo que essas vari√°veis n√£o sejam realmente relevantes. O \(R^2\) ajustado penaliza modelos com muitas vari√°veis independentes que n√£o contribuem significativamente para a explica√ß√£o da variabilidade da vari√°vel dependente. Ele √© uma medida mais conservadora que ajuda a evitar a inclus√£o excessiva de vari√°veis independentes irrelevantes.

Portanto, enquanto o \(R^2\) m√∫ltiplo enfoca a propor√ß√£o de variabilidade explicada pelas vari√°veis independentes, o \(R^2\) ajustado fornece uma medida mais realista e ajustada do ajuste do modelo, levando em considera√ß√£o a complexidade do modelo e o tamanho da amostra.


[[Correla√ß√£o e Colinearidade]]