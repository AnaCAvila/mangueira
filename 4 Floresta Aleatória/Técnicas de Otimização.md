
Stochastic gradient descent
[Avoiding Local Minima When Training A Machine Learning Model With TensorFlow â€“ Surfactants](https://www.surfactants.net/avoiding-local-minima-when-training-a-machine-learning-model-with-tensorflow/)

novograd optimizer
[neural networks - How to avoid falling into the "local minima" trap? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/1362/how-to-avoid-falling-into-the-local-minima-trap)

[Gentle Introduction to the Adam Optimization Algorithm for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
