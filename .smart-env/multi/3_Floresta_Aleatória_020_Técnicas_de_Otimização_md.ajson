"SmartSource:3 Floresta Aleatória/020 Técnicas de Otimização.md": {"path":"3 Floresta Aleatória/020 Técnicas de Otimização.md","class_name":"SmartSource","blocks":{"#":[2,9]},"mtime":1716750111570,"size":644,"hash":"081c490f95cf51d55689077ab966e3096d1227f4d6609495024ccfe13020388c","last_read_hash":"081c490f95cf51d55689077ab966e3096d1227f4d6609495024ccfe13020388c","outlinks":[{"title":"Avoiding Local Minima When Training A Machine Learning Model With TensorFlow – Surfactants","target":"https://www.surfactants.net/avoiding-local-minima-when-training-a-machine-learning-model-with-tensorflow/","line":3},{"title":"neural networks - How to avoid falling into the \"local minima\" trap? - Artificial Intelligence Stack Exchange","target":"https://ai.stackexchange.com/questions/1362/how-to-avoid-falling-into-the-local-minima-trap","line":6},{"title":"Gentle Introduction to the Adam Optimization Algorithm for Deep Learning - MachineLearningMastery.com","target":"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/","line":8}],"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.03573227,0.00916564,0.0225833,0.03098579,0.04023497,0.05656496,-0.04930104,0.02304633,0.06168624,0.01180231,0.03048943,-0.06394259,0.00141817,0.07572787,0.00803381,0.02310872,-0.06603603,0.04322958,0.02484264,-0.00852438,0.06469204,0.00326271,-0.00752896,-0.04657431,0.03430053,-0.00299446,-0.00060464,-0.00064175,-0.02330549,-0.25344467,0.04492456,-0.04710168,-0.03155227,-0.01461224,-0.00300898,0.01058942,0.00416421,0.02325571,-0.10014571,0.02508164,-0.02614724,0.03557015,-0.07113945,-0.0495628,0.02953947,-0.03271576,0.01575295,-0.08052618,-0.07192965,-0.0053507,-0.03951668,-0.02972949,0.00396435,0.00121596,-0.00931501,0.00717921,0.06831932,0.05763406,0.01596272,0.03815252,0.04024418,0.07241675,-0.14797142,0.03607494,0.05130601,0.02541307,-0.02174168,0.00276739,0.05172932,0.0980052,-0.0121125,0.03104377,0.0128806,0.04480799,0.02077075,0.04156102,0.03199581,-0.0146599,0.0478878,-0.03874654,0.05101228,0.04179005,-0.04297404,-0.06932707,-0.02428527,-0.05094869,0.00810878,0.00128805,0.03316277,-0.01815968,-0.02833019,-0.05432788,-0.00342444,-0.00670903,-0.02162495,0.01748137,0.01870664,-0.00693606,-0.09475038,0.12510359,-0.04022052,-0.01067744,-0.00244041,0.01645454,-0.06006865,-0.02370565,-0.04641886,-0.04246966,0.00459986,-0.01336924,0.00998869,-0.02756771,0.02764604,-0.0099486,-0.0156753,-0.01307633,0.02307975,0.09515236,-0.00552953,0.05251635,-0.05318557,-0.00013748,0.09730797,0.01386258,0.03761968,-0.03533465,-0.06789856,0.06161124,0.01043061,0.03544045,0.0468718,-0.03528401,-0.04181777,-0.0733019,0.0312553,-0.02249208,0.02412894,0.00738709,-0.01479291,-0.06082578,-0.03431328,-0.01884353,0.07213451,-0.07946106,-0.05462852,0.11079832,-0.03594009,-0.03989458,-0.02555281,-0.13447706,-0.02081195,0.0024582,-0.07964088,-0.00751661,0.00796951,0.06132262,0.06720834,0.05877492,-0.14344101,0.03438918,-0.06610083,-0.019527,-0.02816735,0.13378605,0.00721825,-0.05127568,-0.05747689,-0.04644926,-0.01335203,-0.00264232,0.07493304,0.0142221,0.00358923,-0.02451167,-0.01028932,0.02482892,-0.13561787,-0.02497683,-0.06613367,-0.00451656,-0.04373292,-0.02949533,0.02836624,-0.01368866,0.00416315,0.02652259,0.04532828,-0.06134238,-0.03252719,-0.00262174,-0.08486623,-0.03063522,-0.0057436,0.0080455,-0.00634945,-0.0325248,0.02496309,0.02119072,-0.00006978,-0.00211484,-0.01495705,0.01890287,0.01998603,0.02363488,0.04731291,-0.0089151,0.04174725,-0.00488002,0.08237413,0.07538048,-0.07052979,-0.02415547,0.03380189,-0.05857963,0.06396733,0.01805221,0.02994174,0.02500112,-0.03826107,0.03976597,0.08673491,0.01933609,-0.05415609,-0.18296701,-0.02263812,0.00064707,0.01425811,0.00575236,-0.09681763,0.05751685,-0.03609106,0.03419193,0.05136158,0.06365482,-0.01913058,-0.01193442,0.02306851,-0.01079356,0.00165035,0.02958532,0.0070215,0.00677822,0.06194588,0.02543535,0.00106493,0.0179779,-0.05279765,-0.00991374,-0.05697052,0.10454065,0.0258292,0.09921941,0.00673253,-0.03761812,0.04235132,-0.01440615,-0.02987372,0.02277235,-0.01477845,0.07079934,-0.02835169,0.02657469,0.00593374,0.00941276,0.07373244,0.00443829,-0.11616609,-0.08718859,-0.06705511,-0.04563357,0.00831757,-0.03313152,-0.02104199,0.01657471,-0.00051464,0.05511867,-0.05016245,0.00702851,-0.04536491,-0.06866244,-0.0024778,-0.03043855,0.02122795,0.01344299,-0.07880824,-0.05197424,-0.05851809,0.05110507,-0.00072037,0.02997987,-0.03342513,0.05571491,0.00885297,0.03962493,0.17156827,0.04336329,-0.00335159,0.03930618,0.00854306,0.07020395,-0.00904891,-0.06525939,-0.00033936,0.03373529,-0.00746836,0.05487764,0.06181045,-0.00534162,0.03055829,0.09443612,-0.0526016,-0.00633435,0.02164033,-0.0094943,0.01335476,-0.06438643,0.03850492,0.04555782,-0.03297605,-0.21391369,0.00979787,0.02605993,0.04710092,0.01784236,0.01499373,0.11000685,-0.00938143,-0.00856122,0.02563698,-0.07792057,0.00651551,0.05755076,0.03056249,0.01745146,0.01203011,0.00441602,0.00241617,0.07542206,-0.0957258,0.03047044,0.07368878,0.17671238,-0.03874983,-0.01958742,0.01597871,-0.03049711,0.02498093,0.04641836,-0.0052355,-0.0364306,0.03786942,0.11648825,-0.00846828,0.06636574,0.07398894,-0.02338315,-0.0103759,0.05686703,-0.01565293,0.00074256,-0.00880044,0.02370504,0.04134675,0.0197892,-0.02230755,-0.03070915,-0.03860783,-0.04362677,0.04371343,0.02456709,-0.0104345,0.00723675,-0.02962996,0.06552531,0.09925633,-0.04197208,-0.06238038,-0.08297994,-0.00740678,0.0385465,-0.06625915,0.00405487,-0.02503039,0.00297472],"tokens":197}}}
"SmartBlock:3 Floresta Aleatória/020 Técnicas de Otimização.md#": {"class_name":"SmartBlock","embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.03364884,0.00603825,0.02454796,0.02979869,0.04312023,0.05315011,-0.0473843,0.02471444,0.05523445,0.01017636,0.03103753,-0.06627046,-0.00192456,0.07495135,0.01802967,0.01788924,-0.0661176,0.04641625,0.02852156,-0.01243806,0.06571627,0.00153714,-0.00613653,-0.05052753,0.03700525,-0.00357123,-0.00202607,-0.00318995,-0.02109792,-0.25307208,0.05291947,-0.04474999,-0.03479837,-0.01497315,-0.00519387,0.00706803,0.00713401,0.02403232,-0.09685577,0.02860459,-0.02611049,0.0353424,-0.07219376,-0.04801218,0.03085159,-0.03330221,0.01359472,-0.08179305,-0.06541366,-0.01083938,-0.03713122,-0.02902975,0.00493604,-0.00028816,-0.00763259,0.002809,0.06905209,0.05996622,0.01305468,0.04329161,0.03710813,0.07230117,-0.14690654,0.04066844,0.0463752,0.03041748,-0.01649808,0.00243527,0.05577838,0.09966315,-0.01100387,0.03456354,0.00252012,0.04684529,0.01491807,0.03869352,0.0311816,-0.01243559,0.05094373,-0.03443148,0.04638832,0.0400183,-0.04377189,-0.07453431,-0.02273504,-0.05075921,0.00811131,0.00279437,0.03237203,-0.02392453,-0.03263275,-0.05562323,-0.00597663,-0.00965392,-0.01169276,0.01394437,0.02445268,-0.00603964,-0.09434832,0.12195176,-0.04017719,-0.0125589,-0.00678255,0.01783341,-0.05882826,-0.02452547,-0.04976525,-0.04379099,0.00444681,-0.01503165,0.00534645,-0.02605464,0.02401086,-0.01074698,-0.01061378,-0.00728046,0.0213847,0.09130595,-0.00670207,0.0531955,-0.05480209,-0.00015257,0.09804557,0.01677912,0.03363989,-0.03930686,-0.0702598,0.05866767,0.0154274,0.03428249,0.04627362,-0.03786891,-0.04444457,-0.07185268,0.04073999,-0.02966246,0.02108459,0.00717705,-0.0061634,-0.05869677,-0.02818694,-0.02361534,0.07231236,-0.07687915,-0.05334962,0.11491934,-0.0359961,-0.04012484,-0.02126499,-0.13377935,-0.02027963,-0.0016498,-0.08045545,-0.0074571,0.01081477,0.05981626,0.073258,0.05979057,-0.14540337,0.02854107,-0.06497749,-0.01990878,-0.02711823,0.13962203,0.00437035,-0.04744747,-0.05590751,-0.04838595,-0.01580943,-0.00108222,0.06707443,0.01220731,0.00979224,-0.02106965,-0.01205895,0.02534796,-0.13563409,-0.0266514,-0.06207269,-0.00577325,-0.04690568,-0.03033681,0.02893999,-0.02119736,0.00366062,0.02172407,0.03652192,-0.0607764,-0.02878104,-0.00715283,-0.08668146,-0.03312971,-0.01019241,0.00351608,-0.00894708,-0.02828067,0.02596113,0.0208997,0.00334893,-0.00651227,-0.0107003,0.01717431,0.02542433,0.02749604,0.04843502,-0.00531336,0.04663492,-0.01303853,0.08422714,0.07148758,-0.06552839,-0.02125297,0.03752854,-0.05906711,0.05990725,0.01622005,0.02812923,0.02582351,-0.0361711,0.04101651,0.0833735,0.02399283,-0.05724613,-0.18377,-0.022621,0.0037696,0.01759882,0.00769873,-0.09961617,0.0589722,-0.03755582,0.02589617,0.04710921,0.0594328,-0.01964998,-0.01711507,0.02336987,-0.01818541,0.00194841,0.0330692,0.00693981,0.00353438,0.05842194,0.02554498,-0.00084197,0.01728736,-0.05103743,-0.00776787,-0.05348639,0.10541132,0.02799535,0.09838052,0.00964575,-0.03491839,0.04133076,-0.01663136,-0.03037464,0.01871691,-0.01182867,0.07256204,-0.0285318,0.02707824,0.00677085,0.00996495,0.07569469,0.00157908,-0.1240768,-0.08334472,-0.06923274,-0.04708282,0.01662749,-0.02905433,-0.01983139,0.02249821,-0.00566735,0.06399719,-0.04636514,0.00810162,-0.04376952,-0.07054372,-0.00149037,-0.03580608,0.02289145,0.01228802,-0.07756333,-0.0501637,-0.05587516,0.05241711,0.00257654,0.02965615,-0.02548972,0.05606096,0.00743687,0.0330354,0.17583792,0.04681093,-0.00423387,0.03975501,0.00925089,0.06223988,-0.00623026,-0.05953308,0.00251596,0.0371355,-0.00497301,0.05718653,0.05312085,-0.00669202,0.02991791,0.0894316,-0.0482159,-0.00001949,0.02607673,-0.0090702,0.01221087,-0.06498654,0.0353584,0.05187095,-0.02949136,-0.21639502,0.00853407,0.0282516,0.04481569,0.01588574,0.01014777,0.10530704,-0.00927965,-0.0138549,0.02481577,-0.08405687,0.01149699,0.05604097,0.0306913,0.01780464,0.01094118,0.00648641,0.00697637,0.07114506,-0.09251456,0.02935074,0.0695967,0.17980246,-0.03275423,-0.02016394,0.01434257,-0.02949053,0.02006224,0.04582844,-0.00809208,-0.03705133,0.04000437,0.11674745,-0.01298742,0.06820699,0.06931086,-0.01812638,-0.00758361,0.05713858,-0.01057708,0.00359224,-0.0120691,0.02541227,0.03855203,0.02510473,-0.02708307,-0.03362786,-0.0372452,-0.04519103,0.04098672,0.02655813,-0.00428743,0.00982632,-0.02673029,0.06116347,0.10030711,-0.03919968,-0.0638532,-0.08412991,-0.01163522,0.04493463,-0.07188623,0.00360579,-0.02588973,0.00409974],"tokens":196}},"key":"3 Floresta Aleatória/020 Técnicas de Otimização.md#","outlinks":[{"title":"Avoiding Local Minima When Training A Machine Learning Model With TensorFlow – Surfactants","target":"https://www.surfactants.net/avoiding-local-minima-when-training-a-machine-learning-model-with-tensorflow/","line":2},{"title":"neural networks - How to avoid falling into the \"local minima\" trap? - Artificial Intelligence Stack Exchange","target":"https://ai.stackexchange.com/questions/1362/how-to-avoid-falling-into-the-local-minima-trap","line":5},{"title":"Gentle Introduction to the Adam Optimization Algorithm for Deep Learning - MachineLearningMastery.com","target":"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/","line":7}],"size":641,"hash":"d8d40a89af0621fdd7bf33901bf0c1461caf013f445c28ad7d870a44c983767e"}